---
title: "Упражнение 4"
author: "Берлина М."
date: "April 13, 2018"
output: html_document
---
Цель: исследовать набор данных "Boston" с помощью линейной регрессионной модели. Задействовав регрессоры "zn", "rm" и "chas", сделать выводы о пригодности модели для прогноза.Сравнить с методом k ближайших соседей по MSE на тестовой выборке. 

```{r setup, echo = F, message = F, warning = F}
library("MASS") #пакет с данными
library("GGally") #графики совместного разброса переменных
library("lmtest") #тесты остатков регрессионных моделей
library("FNN") #алгоритм kNN

knitr::opts_chunk$set(echo = F,        # не выводить сам код
                      message = F,     # не выводить сообщения
                      warning = F)     # не выводить предупреждения

#константы:
my.seed=12345
train.percent=0.85

data(Boston)#открываем данные
?Boston #справка по данным
Boston=subset(Boston[c("crim", "zn", "rm", "chas")]) #отбор требуемых переменных для анализа
Boston$chas=as.factor(Boston$chas) #преобразуем категориальную переменную в факторную

#обучающая выборка:
set.seed(my.seed)
inTrain <- sample(seq_along(Boston$crim), 
                  nrow(Boston) * train.percent)
df.train <- Boston[inTrain, c(colnames(Boston)[-1], colnames(Boston)[1])]
df.test <- Boston[-inTrain, -1]

```

## Описание переменных
Набор данных ```Boston``` содержит переменные:

*```crim``` - количество преступлений на душу населения в пригороде;
*```zn``` - доля спальных районов размером свыше 25 000 кв. футов;
*```rm``` - среднее количество комнат на одного жителя;
*```chas``` - находится на берегу реки Чарльз (1 - находится, 0 - нет)
Размерность обучающей выборки: n=430 строк, p=3 объясняющих переменных. Зависимая переменная - ```crim```.

```{r}
#описательная статистика по переменным:
summary(df.train)
#совместный график разброса переменных:
ggp <- ggpairs(df.train)
print(ggp, progress=F)
#цвета по фактору chas:
ggp <- ggpairs(df.train[, c('zn','rm', 'chas', 'crim')], 
               mapping = ggplot2::aes(color = chas))
print(ggp, progress = F)
```

##Модели:

```{r}
model.1=lm(crim ~ . + chas:rm + chas:zn, data=df.train)
summary(model.1)

```
Совместное влияние zn и chas исключаем, так как параметр статистически незначим:

```{r}
model.2=lm(crim ~ . + chas:rm, data=df.train)
summary(model.2)
```
В данной модели все объясняющие переменные статистически значимые, поэтому мы выбираем её как окончательную Проверим её остатки:

```{r}
#тест Бройша-Пагана:
bptest(model.2)

#статистика Дарбина-Уотсона:
dwtest(model.2)

#графики остатков:
par(mar = c(4.5, 4.5, 2, 1))
par(mfrow = c(1, 3))
plot(model.2, 1)
plot(model.2, 4)
plot(model.2, 5)
```

В данной модели нет автокорреляции остатков, но присутствует гетероскедастичность и влиятельные наблюдения.

#Сравнение с kNN

```{r}
# фактические значения y на тестовой выборке
y.fact <- Boston[-inTrain, 1]
y.model.lm <- predict(model.2, df.test)
MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)

# kNN требует на вход только числовые переменные
df.train.num <- as.data.frame(apply(df.train,2, as.numeric))
df.test.num <- as.data.frame(apply(df.test, 2, as.numeric))

for (i in 2:50){
    model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% 'crim')], 
                     y = df.train.num[, 'crim'], 
                     test = df.test.num, k = i)
    y.model.knn <- model.knn$pred
    if (i == 2){
        MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)
    } else {
        MSE.knn <- c(MSE.knn, 
                     sum((y.model.knn - y.fact)^2) / length(y.model.knn))
    }
}

# график
#par(mar = c(4.5, 4.5, 1, 1))
plot(2:50, MSE.knn, type = 'b', col = 'darkgreen',
     xlab = 'значение k', ylab = 'MSE на тестовой выборке')
lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2)
legend('topright', lty = c(1, 2), pch = c(1, NA), 
       col = c('darkgreen', grey(0.2)), 
       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 
       lwd = rep(2, 2))
```

Как мы можем видеть по графику, ошибка регрессии на тестовой выборке приблизительно совпадает с ошибкой метода k ближайших соседей при k больше 16. Ошибка метода k ближайших соседей практически не меняется при всех k больше 16.

Поскольку ошибка регрессионной модели не больше ошибки метода k ближайших соседей, данную модель можно использовать для прогнозирования.